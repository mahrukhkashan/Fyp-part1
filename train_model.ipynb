{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55070195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /usr/local/lib/python3.12/dist-packages (3.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
      "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.12/dist-packages (2.9.11)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (2.0.45)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.15.0)\n",
      "✓ Libraries imported\n",
      "✓ PhysioNet credentials configured\n",
      "Downloading PATIENTS...\n",
      "--2025-12-15 10:34:31--  https://physionet.org/content/mimiciii/1.4/PATIENTS.csv.gz\n",
      "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
      "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2025-12-15 10:34:32 ERROR 403: Forbidden.\n",
      "\n",
      "Downloading ADMISSIONS...\n",
      "--2025-12-15 10:34:32--  https://physionet.org/content/mimiciii/1.4/ADMISSIONS.csv.gz\n",
      "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
      "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2025-12-15 10:34:32 ERROR 403: Forbidden.\n",
      "\n",
      "Downloading DIAGNOSES_ICD...\n",
      "--2025-12-15 10:34:32--  https://physionet.org/content/mimiciii/1.4/DIAGNOSES_ICD.csv.gz\n",
      "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
      "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2025-12-15 10:34:33 ERROR 403: Forbidden.\n",
      "\n",
      "Files now in mimic_data: ['PATIENTS.csv.gz', 'ADMISSIONS.csv.gz', 'DIAGNOSES_ICD.csv.gz']\n",
      "Downloading MIMIC-III files from PhysioNet...\n",
      "✓ PATIENTS already exists\n",
      "✓ ADMISSIONS already exists\n",
      "✓ DIAGNOSES_ICD already exists\n",
      "\n",
      "✓ All files downloaded\n",
      "Loading data...\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m_infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m_buffered_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m_next_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                 \u001b[0morig_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_iter_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m_next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0;31m# for mypy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1825373867.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mpatients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mimic_data/PATIENTS.csv.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0madmissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mimic_data/ADMISSIONS.csv.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mdiagnoses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mimic_data/DIAGNOSES_ICD.csv.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_original_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         ) = self._infer_columns()\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# Now self.columns has the set of columns that we will process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m_infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmptyDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No columns to parse from file\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Sepsis Prediction Model Training with MIMIC-III on PhysioNet\n",
    "# \n",
    "# This notebook trains sepsis prediction models using MIMIC-III data directly from PhysioNet.\n",
    "# \n",
    "# **Credentials Required**: You need valid PhysioNet credentials (apply at https://physionet.org/).\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1: Setup and Authentication\n",
    "\n",
    "# %%\n",
    "# Install required packages\n",
    "!pip install wget pandas numpy scikit-learn xgboost shap imbalanced-learn plotly seaborn matplotlib scipy\n",
    "!pip install psycopg2-binary sqlalchemy  # Optional: if you want to connect to PostgreSQL\n",
    "\n",
    "# %%\n",
    "# Import libraries\n",
    "import os\n",
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, confusion_matrix, \n",
    "                           classification_report, roc_curve, auc)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✓ Libraries imported\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2: Configure PhysioNet Credentials\n",
    "\n",
    "# %%\n",
    "# Configure PhysioNet credentials\n",
    "# Replace with your actual PhysioNet credentials\n",
    "PHYSIONET_USERNAME = \"nazeefulhaq\"  # Replace with your PhysioNet username\n",
    "PHYSIONET_PASSWORD = \"Nazeef161haq#\"  # Replace with your PhysioNet password\n",
    "\n",
    "# Create .netrc file for authentication\n",
    "netrc_content = f\"\"\"\n",
    "machine physionet.org\n",
    "login {PHYSIONET_USERNAME}\n",
    "password {PHYSIONET_PASSWORD}\n",
    "\"\"\"\n",
    "\n",
    "# Save credentials\n",
    "with open('/root/.netrc', 'w') as f:\n",
    "    f.write(netrc_content)\n",
    "\n",
    "print(\"✓ PhysioNet credentials configured\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3: Download Required MIMIC-III Files\n",
    "\n",
    "# %%\n",
    "# Define which files to download (we'll start with essential tables)\n",
    "MIMIC_FILES = {\n",
    "    \"PATIENTS\": \"https://physionet.org/content/mimiciii/1.4/PATIENTS.csv.gz\",\n",
    "    \"ADMISSIONS\": \"https://physionet.org/content/mimiciii/1.4/ADMISSIONS.csv.gz\",\n",
    "    \"DIAGNOSES_ICD\": \"https://physionet.org/content/mimiciii/1.4/DIAGNOSES_ICD.csv.gz\",\n",
    "    #\"D_ICD_DIAGNOSES\": \"https://physionet.org/content/mimiciii/1.4/D_ICD_DIAGNOSES.csv.gz\",\n",
    "    # Optional: add more tables as needed\n",
    "    # \"CHARTEVENTS\": \"https://physionet.org/files/mimiciii/1.4/CHARTEVENTS.csv.gz\",\n",
    "    # \"LABEVENTS\": \"https://physionet.org/files/mimiciii/1.4/LABEVENTS.csv.gz\",\n",
    "    # \"D_ITEMS\": \"https://physionet.org/files/mimiciii/1.4/D_ITEMS.csv.gz\",\n",
    "    # \"D_LABITEMS\": \"https://physionet.org/files/mimiciii/1.4/D_LABITEMS.csv.gz\",\n",
    "}\n",
    "\n",
    "os.makedirs('mimic_data', exist_ok=True)\n",
    "\n",
    "for name, url in MIMIC_FILES.items():\n",
    "    output_path = f\"mimic_data/{name}.csv.gz\"\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"Downloading {name}...\")\n",
    "        !wget --user {PHYSIONET_USERNAME} --password {PHYSIONET_PASSWORD} {url} -O {output_path}\n",
    "    else:\n",
    "        print(f\"{name} already exists\")\n",
    "\n",
    "# Check the folder contents\n",
    "print(\"Files now in mimic_data:\", os.listdir('mimic_data'))\n",
    "\n",
    "# Download files\n",
    "print(\"Downloading MIMIC-III files from PhysioNet...\")\n",
    "for file_name, url in MIMIC_FILES.items():\n",
    "    output_path = f\"mimic_data/{file_name}.csv.gz\"\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"Downloading {file_name}...\")\n",
    "        try:\n",
    "            wget.download(url, output_path)\n",
    "            print(f\"\\n✓ {file_name} downloaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error downloading {file_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"✓ {file_name} already exists\")\n",
    "\n",
    "print(\"\\n✓ All files downloaded\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4: Load and Process Data\n",
    "\n",
    "# %%\n",
    "# Load essential tables\n",
    "print(\"Loading data...\")\n",
    "\n",
    "patients = pd.read_csv('mimic_data/PATIENTS.csv.gz', compression='gzip', engine='python')\n",
    "admissions = pd.read_csv('mimic_data/ADMISSIONS.csv.gz', compression='gzip', engine='python')\n",
    "diagnoses = pd.read_csv('mimic_data/DIAGNOSES_ICD.csv.gz', compression='gzip', engine='python')\n",
    "# icd_diagnoses = pd.read_csv('mimic_data/D_ICD_DIAGNOSES.csv.gz', compression='gzip', engine='python')\n",
    "\n",
    "print(f\"Patients: {patients.shape}\")\n",
    "print(f\"Admissions: {admissions.shape}\")\n",
    "print(f\"Diagnoses: {diagnoses.shape}\")\n",
    "# print(f\"ICD Diagnoses Dictionary: {icd_diagnoses.shape}\")\n",
    "\n",
    "# Show column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(\"Patients:\", patients.columns.tolist())\n",
    "print(\"Admissions:\", admissions.columns.tolist())\n",
    "print(\"Diagnoses:\", diagnoses.columns.tolist())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5: Create Sepsis Labels\n",
    "\n",
    "# %%\n",
    "# Define sepsis ICD-9 codes (standard sepsis codes)\n",
    "SEPSIS_ICD_CODES = [\n",
    "    '038',       # Septicemia\n",
    "    '038.0',     # Streptococcal septicemia\n",
    "    '038.1',     # Staphylococcal septicemia\n",
    "    '038.2',     # Pneumococcal septicemia\n",
    "    '038.3',     # Septicemia due to anaerobes\n",
    "    '038.4',     # Septicemia due to other gram-negative organisms\n",
    "    '038.40',    # Septicemia due to gram-negative organism, unspecified\n",
    "    '038.41',    # Septicemia due to hemophilus influenzae\n",
    "    '038.42',    # Septicemia due to escherichia coli\n",
    "    '038.43',    # Septicemia due to pseudomonas\n",
    "    '038.44',    # Septicemia due to serratia\n",
    "    '038.49',    # Septicemia due to other gram-negative organisms\n",
    "    '038.8',     # Other specified septicemias\n",
    "    '038.9',     # Unspecified septicemia\n",
    "    '785.52',    # Septic shock\n",
    "    '995.91',    # Sepsis\n",
    "    '995.92'     # Severe sepsis\n",
    "]\n",
    "\n",
    "# Filter sepsis diagnoses\n",
    "sepsis_diagnoses = diagnoses[diagnoses['ICD9_CODE'].isin(SEPSIS_ICD_CODES)]\n",
    "\n",
    "# Create sepsis label for admissions\n",
    "admissions_with_sepsis = admissions.copy()\n",
    "admissions_with_sepsis['SEPSIS_LABEL'] = admissions_with_sepsis.apply(\n",
    "    lambda row: 1 if ((row['SUBJECT_ID'], row['HADM_ID']) in \n",
    "                     zip(sepsis_diagnoses['SUBJECT_ID'], sepsis_diagnoses['HADM_ID'])) else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Total admissions: {len(admissions_with_sepsis)}\")\n",
    "print(f\"Sepsis admissions: {admissions_with_sepsis['SEPSIS_LABEL'].sum()}\")\n",
    "print(f\"Sepsis percentage: {admissions_with_sepsis['SEPSIS_LABEL'].mean()*100:.2f}%\")\n",
    "\n",
    "# Merge with patient data\n",
    "admissions_patients = pd.merge(\n",
    "    admissions_with_sepsis,\n",
    "    patients[['SUBJECT_ID', 'GENDER', 'DOB']],\n",
    "    on='SUBJECT_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged data shape: {admissions_patients.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6: Feature Engineering\n",
    "\n",
    "# %%\n",
    "print(\"Engineering features...\")\n",
    "\n",
    "# Convert dates\n",
    "admissions_patients['ADMITTIME'] = pd.to_datetime(admissions_patients['ADMITTIME'])\n",
    "admissions_patients['DISCHTIME'] = pd.to_datetime(admissions_patients['DISCHTIME'])\n",
    "admissions_patients['DOB'] = pd.to_datetime(admissions_patients['DOB'])\n",
    "\n",
    "# Calculate age\n",
    "admissions_patients['AGE'] = (admissions_patients['ADMITTIME'] - admissions_patients['DOB']).dt.days / 365.25\n",
    "admissions_patients['AGE'] = admissions_patients['AGE'].clip(18, 100)  # Cap unrealistic ages\n",
    "\n",
    "# Calculate length of stay\n",
    "admissions_patients['LOS_DAYS'] = (admissions_patients['DISCHTIME'] - admissions_patients['ADMITTIME']).dt.days\n",
    "\n",
    "# Extract features from existing columns\n",
    "admissions_patients['IS_EMERGENCY'] = (admissions_patients['ADMISSION_TYPE'] == 'EMERGENCY').astype(int)\n",
    "admissions_patients['IS_URGENT'] = (admissions_patients['ADMISSION_TYPE'] == 'URGENT').astype(int)\n",
    "admissions_patients['IS_ELECTIVE'] = (admissions_patients['ADMISSION_TYPE'] == 'ELECTIVE').astype(int)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_cols = ['GENDER', 'ADMISSION_LOCATION', 'INSURANCE', 'LANGUAGE', \n",
    "                    'RELIGION', 'MARITAL_STATUS', 'ETHNICITY', 'DISCHARGE_LOCATION']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in admissions_patients.columns:\n",
    "        le = LabelEncoder()\n",
    "        admissions_patients[col + '_ENCODED'] = le.fit_transform(admissions_patients[col].fillna('Unknown'))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"Features created. Total columns: {len(admissions_patients.columns)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7: Prepare Training Data\n",
    "\n",
    "# %%\n",
    "# Select features for training\n",
    "feature_columns = [\n",
    "    'AGE',\n",
    "    'LOS_DAYS',\n",
    "    'IS_EMERGENCY',\n",
    "    'IS_URGENT', \n",
    "    'IS_ELECTIVE',\n",
    "    'HOSPITAL_EXPIRE_FLAG'\n",
    "] + [col for col in admissions_patients.columns if '_ENCODED' in col]\n",
    "\n",
    "# Remove any columns that might not exist\n",
    "feature_columns = [col for col in feature_columns if col in admissions_patients.columns]\n",
    "\n",
    "print(f\"Selected {len(feature_columns)} features:\")\n",
    "print(feature_columns)\n",
    "\n",
    "# Prepare X and y\n",
    "X = admissions_patients[feature_columns].copy()\n",
    "y = admissions_patients['SEPSIS_LABEL'].copy()\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Sepsis percentage: {y.mean()*100:.2f}%\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "missing_before = X.isnull().sum().sum()\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "missing_after = X.isnull().sum().sum()\n",
    "\n",
    "print(f\"Missing values: {missing_before} → {missing_after}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"✓ Data preparation complete\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 8: Handle Class Imbalance\n",
    "\n",
    "# %%\n",
    "print(f\"Original class distribution: {pd.Series(y).value_counts().to_dict()}\")\n",
    "\n",
    "# Use SMOTE for balanced training\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.5)  # Balance to 1:2 ratio\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(f\"\\nAfter SMOTE:\")\n",
    "print(f\"  X shape: {X_resampled.shape}\")\n",
    "print(f\"  Class distribution: {pd.Series(y_resampled).value_counts().to_dict()}\")\n",
    "print(f\"  Sepsis percentage: {y_resampled.mean()*100:.2f}%\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 9: Train-Test Split\n",
    "\n",
    "# %%\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_resampled\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "print(f\"\\nTraining sepsis: {y_train.sum()} ({y_train.mean()*100:.1f}%)\")\n",
    "print(f\"Testing sepsis: {y_test.sum()} ({y_test.mean()*100:.1f}%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 10: Model Training with GPU Acceleration\n",
    "\n",
    "# %%\n",
    "# Enable GPU for XGBoost if available\n",
    "import sys\n",
    "xgboost_params = {\n",
    "    'n_estimators': 300,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'use_label_encoder': False,\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# Check for GPU\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        xgboost_params['tree_method'] = 'gpu_hist'\n",
    "        xgboost_params['predictor'] = 'gpu_predictor'\n",
    "        print(\"✓ GPU detected. Using GPU acceleration for XGBoost\")\n",
    "    else:\n",
    "        xgboost_params['tree_method'] = 'hist'\n",
    "        print(\"ℹ No GPU detected. Using CPU for training\")\n",
    "except:\n",
    "    xgboost_params['tree_method'] = 'hist'\n",
    "    print(\"ℹ GPU check failed. Using CPU for training\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'XGBoost': XGBClassifier(**xgboost_params),\n",
    "    \n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        C=0.1,\n",
    "        penalty='l2',\n",
    "        solver='liblinear',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"\\nTraining models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "best_model_name = ''\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Track best model\n",
    "    if metrics['f1_score'] > best_score:\n",
    "        best_score = metrics['f1_score']\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Best model: {best_model_name} with F1-Score: {best_score:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 11: Model Evaluation and Visualization\n",
    "\n",
    "# %%\n",
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'{name}\\nF1: {result[\"metrics\"][\"f1_score\"]:.3f}')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=colors[idx], lw=2, \n",
    "             label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Model Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 12: Feature Importance Analysis\n",
    "\n",
    "# %%\n",
    "# Get feature importance from best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_n = min(15, len(feature_importance))\n",
    "    sns.barplot(x='importance', y='feature', \n",
    "                data=feature_importance.head(top_n))\n",
    "    plt.title(f'Top {top_n} Feature Importance - {best_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "    print(\"✓ Feature importance saved to feature_importance.csv\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 13: SHAP Explainability\n",
    "\n",
    "# %%\n",
    "print(\"Creating SHAP explanations...\")\n",
    "\n",
    "try:\n",
    "    # Use a sample for SHAP (for speed)\n",
    "    X_sample = X_train.iloc[:100]\n",
    "    \n",
    "    # Create explainer based on model type\n",
    "    if best_model_name in ['XGBoost', 'Random Forest', 'Gradient Boosting']:\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "        plt.title(f'SHAP Feature Importance - {best_model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save SHAP explainer\n",
    "        with open('shap_explainer.pkl', 'wb') as f:\n",
    "            pickle.dump(explainer, f)\n",
    "        print(\"✓ SHAP explainer saved\")\n",
    "        \n",
    "    else:\n",
    "        # For non-tree models, use KernelExplainer\n",
    "        explainer = shap.KernelExplainer(best_model.predict_proba, X_sample)\n",
    "        shap_values = explainer.shap_values(X_sample.iloc[:5])  # Small sample for speed\n",
    "        print(\"ℹ Using KernelExplainer (small sample for speed)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ℹ SHAP explanation skipped: {e}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 14: Save Model Artifacts\n",
    "\n",
    "# %%\n",
    "# Create directory for model artifacts\n",
    "os.makedirs('model_artifacts', exist_ok=True)\n",
    "\n",
    "# Save all artifacts\n",
    "artifacts = {\n",
    "    'best_model': best_model,\n",
    "    'best_model_name': best_model_name,\n",
    "    'scaler': scaler,\n",
    "    'imputer': imputer,\n",
    "    'feature_columns': feature_columns,\n",
    "    'label_encoders': label_encoders,\n",
    "    'results': results,\n",
    "    'feature_importance': feature_importance if 'feature_importance' in locals() else None\n",
    "}\n",
    "\n",
    "# Save individual files\n",
    "with open('model_artifacts/best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "with open('model_artifacts/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('model_artifacts/imputer.pkl', 'wb') as f:\n",
    "    pickle.dump(imputer, f)\n",
    "\n",
    "with open('model_artifacts/feature_columns.json', 'w') as f:\n",
    "    json.dump(feature_columns, f)\n",
    "\n",
    "with open('model_artifacts/label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "print(\"✓ Model artifacts saved:\")\n",
    "print(\"  1. best_model.pkl - Trained model\")\n",
    "print(\"  2. scaler.pkl - Feature scaler\")\n",
    "print(\"  3. imputer.pkl - Missing value imputer\")\n",
    "print(\"  4. feature_columns.json - Feature names\")\n",
    "print(\"  5. label_encoders.pkl - Categorical encoders\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 15: Create Inference Script for VS Code\n",
    "\n",
    "# %%\n",
    "# Create inference script\n",
    "inference_script = '''\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class SepsisPredictor:\n",
    "    \"\"\"Pre-trained sepsis prediction model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir=\"model_artifacts/\"):\n",
    "        self.model_dir = model_dir\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.imputer = None\n",
    "        self.feature_columns = None\n",
    "        self.label_encoders = None\n",
    "        self.is_loaded = False\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load all model artifacts\"\"\"\n",
    "        try:\n",
    "            # Load model\n",
    "            with open(os.path.join(self.model_dir, 'best_model.pkl'), 'rb') as f:\n",
    "                self.model = pickle.load(f)\n",
    "            \n",
    "            # Load scaler\n",
    "            with open(os.path.join(self.model_dir, 'scaler.pkl'), 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "            \n",
    "            # Load imputer\n",
    "            with open(os.path.join(self.model_dir, 'imputer.pkl'), 'rb') as f:\n",
    "                self.imputer = pickle.load(f)\n",
    "            \n",
    "            # Load feature columns\n",
    "            with open(os.path.join(self.model_dir, 'feature_columns.json'), 'r') as f:\n",
    "                self.feature_columns = json.load(f)\n",
    "            \n",
    "            # Load label encoders\n",
    "            with open(os.path.join(self.model_dir, 'label_encoders.pkl'), 'rb') as f:\n",
    "                self.label_encoders = pickle.load(f)\n",
    "            \n",
    "            self.is_loaded = True\n",
    "            print(\"✓ Sepsis prediction model loaded successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_input(self, patient_data):\n",
    "        \"\"\"Preprocess patient data for prediction\"\"\"\n",
    "        if isinstance(patient_data, dict):\n",
    "            df = pd.DataFrame([patient_data])\n",
    "        else:\n",
    "            df = patient_data.copy()\n",
    "        \n",
    "        # Ensure all features exist\n",
    "        for feature in self.feature_columns:\n",
    "            if feature not in df.columns:\n",
    "                # Provide sensible defaults\n",
    "                if feature == 'AGE':\n",
    "                    df[feature] = 50\n",
    "                elif feature == 'LOS_DAYS':\n",
    "                    df[feature] = 5\n",
    "                elif feature.startswith('IS_'):\n",
    "                    df[feature] = 0\n",
    "                elif feature.endswith('_ENCODED'):\n",
    "                    # Try to encode categorical values\n",
    "                    original_col = feature.replace('_ENCODED', '')\n",
    "                    if original_col in df.columns and original_col in self.label_encoders:\n",
    "                        try:\n",
    "                            df[feature] = self.label_encoders[original_col].transform(\n",
    "                                df[original_col].fillna('Unknown')\n",
    "                            )\n",
    "                        except:\n",
    "                            df[feature] = 0\n",
    "                    else:\n",
    "                        df[feature] = 0\n",
    "                else:\n",
    "                    df[feature] = 0\n",
    "        \n",
    "        # Reorder columns to match training\n",
    "        df = df[self.feature_columns]\n",
    "        \n",
    "        # Apply preprocessing pipeline\n",
    "        df_imputed = self.imputer.transform(df)\n",
    "        df_scaled = self.scaler.transform(df_imputed)\n",
    "        \n",
    "        return df_scaled\n",
    "    \n",
    "    def predict(self, patient_data):\n",
    "        \"\"\"Make sepsis prediction\"\"\"\n",
    "        if not self.is_loaded:\n",
    "            raise Exception(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        # Preprocess\n",
    "        processed_data = self.preprocess_input(patient_data)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.model.predict(processed_data)[0]\n",
    "        probability = self.model.predict_proba(processed_data)[0][1]\n",
    "        \n",
    "        # Determine risk level\n",
    "        risk_level = self._get_risk_level(probability)\n",
    "        \n",
    "        return {\n",
    "            'prediction': int(prediction),\n",
    "            'probability': float(probability),\n",
    "            'risk_level': risk_level,\n",
    "            'recommendation': self._get_recommendation(risk_level),\n",
    "            'confidence': self._get_confidence(probability)\n",
    "        }\n",
    "    \n",
    "    def _get_risk_level(self, probability):\n",
    "        if probability >= 0.7:\n",
    "            return \"High\"\n",
    "        elif probability >= 0.4:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Low\"\n",
    "    \n",
    "    def _get_recommendation(self, risk_level):\n",
    "        recommendations = {\n",
    "            \"High\": \"Immediate medical attention required. Consider ICU admission.\",\n",
    "            \"Medium\": \"Close monitoring recommended. Repeat assessment in 6 hours.\",\n",
    "            \"Low\": \"Regular monitoring. Follow clinical protocols.\"\n",
    "        }\n",
    "        return recommendations.get(risk_level, \"Consult physician.\")\n",
    "    \n",
    "    def _get_confidence(self, probability):\n",
    "        if probability > 0.8 or probability < 0.2:\n",
    "            return \"High\"\n",
    "        elif probability > 0.6 or probability < 0.4:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Low\"\n",
    "    \n",
    "    def get_feature_importance(self, top_n=10):\n",
    "        \"\"\"Get top N important features\"\"\"\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            importance = dict(zip(self.feature_columns, self.model.feature_importances_))\n",
    "            sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "            return sorted_importance[:top_n]\n",
    "        return None\n",
    "\n",
    "# Singleton instance for easy access\n",
    "sepsis_predictor = SepsisPredictor()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    predictor = SepsisPredictor(\"model_artifacts/\")\n",
    "    if predictor.load_model():\n",
    "        # Example patient\n",
    "        sample_patient = {\n",
    "            'AGE': 65,\n",
    "            'LOS_DAYS': 3,\n",
    "            'IS_EMERGENCY': 1,\n",
    "            'IS_URGENT': 0,\n",
    "            'IS_ELECTIVE': 0,\n",
    "            'HOSPITAL_EXPIRE_FLAG': 0,\n",
    "            'GENDER_ENCODED': 1,\n",
    "            'ADMISSION_LOCATION_ENCODED': 2,\n",
    "            'INSURANCE_ENCODED': 1,\n",
    "            'ETHNICITY_ENCODED': 3\n",
    "        }\n",
    "        \n",
    "        result = predictor.predict(sample_patient)\n",
    "        print(f\"Prediction: {result}\")\n",
    "'''\n",
    "\n",
    "# Save inference script\n",
    "with open('sepsis_predictor.py', 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(\"✓ Inference script created: sepsis_predictor.py\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 16: Create Complete Model Package\n",
    "\n",
    "# %%\n",
    "# Create a zip file with everything needed\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Files to include in the package\n",
    "files_to_zip = [\n",
    "    'model_artifacts/best_model.pkl',\n",
    "    'model_artifacts/scaler.pkl',\n",
    "    'model_artifacts/imputer.pkl',\n",
    "    'model_artifacts/feature_columns.json',\n",
    "    'model_artifacts/label_encoders.pkl',\n",
    "    'sepsis_predictor.py',\n",
    "    'feature_importance.csv'\n",
    "]\n",
    "\n",
    "# Filter existing files\n",
    "existing_files = [f for f in files_to_zip if os.path.exists(f)]\n",
    "\n",
    "# Create zip\n",
    "with zipfile.ZipFile('sepsis_model_package.zip', 'w') as zipf:\n",
    "    for file in existing_files:\n",
    "        zipf.write(file, arcname=os.path.basename(file))  # Keep filenames clean\n",
    "print(\"✓ Model package created: sepsis_model_package.zip\")\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
